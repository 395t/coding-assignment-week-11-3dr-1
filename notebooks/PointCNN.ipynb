{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PointCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MOeP7z0_yEq"
      },
      "source": [
        "# PointCNN Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uw6nS7L_vHM"
      },
      "source": [
        "## Package Installation, Imports, Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AP2Eswi7hzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c683f946-3872-4e6c-b8b8-5dab2072f0e4"
      },
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install -q torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install -q torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install -q torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install -q torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install -q torch-geometric"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 10.4 MB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 870 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 407 kB 40.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.0 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsIpzvDWtA-Q"
      },
      "source": [
        "import os.path as osp\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear as Lin\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torch_geometric.datasets import ModelNet, ShapeNet\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import XConv, fps, global_mean_pool\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import intersection_and_union as i_and_u"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODldekGC-HsZ"
      },
      "source": [
        "def get_dataset(name, num_points):\n",
        "    path = osp.join('data', name)\n",
        "    pre_transform = T.NormalizeScale()\n",
        "\n",
        "    if name == 'ModelNet10':\n",
        "      transform = T.SamplePoints(num_points)\n",
        "\n",
        "      train_dataset = ModelNet(\n",
        "          path,\n",
        "          name='10',\n",
        "          train=True,\n",
        "          transform=transform,\n",
        "          pre_transform=pre_transform)\n",
        "      test_dataset = ModelNet(\n",
        "          path,\n",
        "          name='10',\n",
        "          train=False,\n",
        "          transform=transform,\n",
        "          pre_transform=pre_transform)\n",
        "    elif name == 'ModelNet40':     \n",
        "      transform = T.SamplePoints(num_points)\n",
        "\n",
        "      train_dataset = ModelNet(\n",
        "          path,\n",
        "          name='40',\n",
        "          train=True,\n",
        "          transform=transform,\n",
        "          pre_transform=pre_transform)\n",
        "      test_dataset = ModelNet(\n",
        "          path,\n",
        "          name='40',\n",
        "          train=False,\n",
        "          transform=transform,\n",
        "          pre_transform=pre_transform)\n",
        "    elif name == 'ShapeNet':\n",
        "      category = None  # Pass in `None` to train on all categories.\n",
        "      #transform = T.SamplePoints(num_points)\n",
        "      transform = T.Compose([\n",
        "          T.RandomTranslate(0.01),\n",
        "          T.RandomRotate(15, axis=0),\n",
        "          T.RandomRotate(15, axis=1),\n",
        "          T.RandomRotate(15, axis=2)\n",
        "      ])\n",
        "\n",
        "      train_dataset = ShapeNet(\n",
        "          path,\n",
        "          category,\n",
        "          split='trainval',\n",
        "          transform=transform,\n",
        "          pre_transform=pre_transform)\n",
        "      test_dataset = ShapeNet(\n",
        "          path,\n",
        "          category,\n",
        "          split='test',\n",
        "          transform=transform,\n",
        "          pre_transform=pre_transform)\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiovz43fzrHf"
      },
      "source": [
        "## Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp_dS0oV-a1e"
      },
      "source": [
        "MODELNET_VERSION = 'ModelNet40' # ModelNet10 or ModelNet40\n",
        "NUM_POINTS = [512, 768, 1024]\n",
        "\n",
        "EPOCHS = 100 #200\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "LR_DECAY_FACTOR = 0.5\n",
        "LR_DECAY_STEP_SIZE = 50\n",
        "WEIGHT_DECAY = 0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gN8Bfv2C-FZ"
      },
      "source": [
        "def train(model, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.to(device)\n",
        "        out = model(data.pos, data.batch)\n",
        "        loss = F.nll_loss(out, data.y)\n",
        "        losses.append(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return sum(losses)/len(losses)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.pos, data.batch)\n",
        "        loss = F.nll_loss(out, data.y)\n",
        "        losses.append(loss)\n",
        "        pred = model(data.pos, data.batch).max(1)[1]\n",
        "        correct += pred.eq(data.y).sum().item()\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "\n",
        "    return test_acc, sum(losses)/len(losses)\n",
        "\n",
        "\n",
        "def run(train_dataset, test_dataset, model, epochs, batch_size, lr,\n",
        "        lr_decay_factor, lr_decay_step_size, weight_decay):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  \n",
        "    duration = 0\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_accs = []\n",
        "    model = model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_start = time.perf_counter()\n",
        "\n",
        "        train_loss = train(model, optimizer, train_loader, device)\n",
        "        train_losses.append(train_loss.item())\n",
        "        test_acc, test_loss = test(model, test_loader, device)\n",
        "        #test_loss = test(model, test_loader, device)\n",
        "        test_accs.append(test_acc)\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_end = time.perf_counter()\n",
        "        duration += t_end - t_start\n",
        "        print('Epoch: {:03d}, Test: {:.4f}, Duration: {:.2f}'.format(\n",
        "            epoch, test_acc, t_end - t_start))\n",
        "\n",
        "        if epoch % lr_decay_step_size == 0:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_decay_factor * param_group['lr']\n",
        "\n",
        "    return duration, train_losses, test_losses, test_accs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-xi7to_xBBc"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = XConv(0, 48, dim=3, kernel_size=8, hidden_channels=32)\n",
        "        self.conv2 = XConv(48, 96, dim=3, kernel_size=12, hidden_channels=64, dilation=2)\n",
        "        self.conv3 = XConv(96, 192, dim=3, kernel_size=16, hidden_channels=128, dilation=2)\n",
        "        self.conv4 = XConv(192, 384, dim=3, kernel_size=16, hidden_channels=256, dilation=3)\n",
        "\n",
        "        self.lin1 = Lin(384, 256)\n",
        "        self.lin2 = Lin(256, 128)\n",
        "        self.lin3 = Lin(128, num_classes)\n",
        "\n",
        "    def forward(self, pos, batch):\n",
        "        x = F.relu(self.conv1(None, pos, batch))\n",
        "\n",
        "        idx = fps(pos, batch, ratio=0.375)\n",
        "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
        "\n",
        "        x = F.relu(self.conv2(x, pos, batch))\n",
        "\n",
        "        idx = fps(pos, batch, ratio=0.334)\n",
        "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
        "\n",
        "        x = F.relu(self.conv3(x, pos, batch))\n",
        "        x = F.relu(self.conv4(x, pos, batch))\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin3(x)\n",
        "        return F.log_softmax(x, dim=-1)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJFmoHWP4LL7"
      },
      "source": [
        "results = dict()\n",
        "for POINTS in NUM_POINTS:\n",
        "  train_dataset, test_dataset = get_dataset(name=MODELNET_VERSION, num_points=POINTS)\n",
        "  model = Net(train_dataset.num_classes)\n",
        "\n",
        "  duration, train_losses, test_losses, test_accs = run(train_dataset, test_dataset, model, EPOCHS, BATCH_SIZE, LR, LR_DECAY_FACTOR, LR_DECAY_STEP_SIZE, WEIGHT_DECAY)\n",
        "\n",
        "  results[str(POINTS)+'_duration'] = duration\n",
        "  results[str(POINTS)+'_train_losses'] = train_losses\n",
        "  results[str(POINTS)+'_test_losses'] = test_losses\n",
        "  results[str(POINTS)+'_test_accs'] = test_accs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRtGHfpqVlp4"
      },
      "source": [
        "results.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FHMsrNu__4w"
      },
      "source": [
        "plt.plot(range(1,EPOCHS+1), results['512_train_losses'], label='512')\n",
        "plt.plot(range(1,EPOCHS+1), results['768_train_losses'], label='768')\n",
        "plt.plot(range(1,EPOCHS+1), results['1024_train_losses'], label='1024')\n",
        "plt.title(MODELNET_VERSION + ' Train Losses varying Num Points')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAajyW9OC6UR"
      },
      "source": [
        "plt.plot(range(1,EPOCHS+1), results['512_test_losses'], label='512')\n",
        "plt.plot(range(1,EPOCHS+1), results['768_test_losses'], label='768')\n",
        "plt.plot(range(1,EPOCHS+1), results['1024_test_losses'], label='1024')\n",
        "plt.title(MODELNET_VERSION + ' Test Losses varying Num Points')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qb3wr7Q7fzh"
      },
      "source": [
        "plt.plot(range(1,EPOCHS+1), results['512_test_accs'], label='512')\n",
        "plt.plot(range(1,EPOCHS+1), results['768_test_accs'], label='768')\n",
        "plt.plot(range(1,EPOCHS+1), results['1024_test_accs'], label='1024')\n",
        "plt.title(MODELNET_VERSION + ' Test Accuracies varying Num Points')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2PYt1ea8YK5"
      },
      "source": [
        "print(results['512_duration'], results['768_duration'], results['1024_duration'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGnyfZnm_sj6"
      },
      "source": [
        "print(sum(results['512_test_accs'])/len(results['512_test_accs']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWyOYmtZAPyh"
      },
      "source": [
        "print(sum(results['768_test_accs'])/len(results['768_test_accs']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn8Xa0wTAPeq"
      },
      "source": [
        "print(sum(results['1024_test_accs'])/len(results['1024_test_accs']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sLld90gzxXb"
      },
      "source": [
        "## Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S1CNofPrgVM"
      },
      "source": [
        "EPOCHS = 5 #200\n",
        "BATCH_SIZE = 81931 #32\n",
        "LR = 0.001\n",
        "LR_DECAY_FACTOR = 0.5\n",
        "LR_DECAY_STEP_SIZE = 50\n",
        "WEIGHT_DECAY = 0"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INZEbqlTsZoh"
      },
      "source": [
        "def train(model, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    total_loss = correct_nodes = total_nodes = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.pos, data.batch)\n",
        "        loss = F.nll_loss(out, data.y)\n",
        "        losses.append(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct_nodes += out.argmax(dim=1).eq(data.y).sum().item()\n",
        "        total_nodes += data.num_nodes\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'[{i+1}/{len(train_loader)}] Loss: {total_loss / 10:.4f} '\n",
        "                  f'Train Acc: {correct_nodes / total_nodes:.4f}')\n",
        "            total_loss = correct_nodes = total_nodes = 0\n",
        "\n",
        "    return sum(losses)/len(losses)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    accs = []\n",
        "    losses = []\n",
        "    y_mask = loader.dataset.y_mask\n",
        "    ious = [[] for _ in range(len(loader.dataset.categories))]\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data)\n",
        "        #pred = model(data).argmax(dim=1)\n",
        "\n",
        "        correct_nodes += out.argmax(dim=1).eq(data.y).sum().item()\n",
        "        total_nodes += data.num_nodes\n",
        "        accs.append(correct_nodes / total_nodes)\n",
        "\n",
        "        loss = F.nll_loss(out, data.y)\n",
        "        losses.append(loss)\n",
        "\n",
        "        pred = out.argmax(dim=1)\n",
        "        i, u = i_and_u(pred, data.y, loader.dataset.num_classes, data.batch)\n",
        "        iou = i.cpu().to(torch.float) / u.cpu().to(torch.float)\n",
        "        iou[torch.isnan(iou)] = 1\n",
        "\n",
        "        # Find and filter the relevant classes for each category.\n",
        "        for iou, category in zip(iou.unbind(), data.category.unbind()):\n",
        "            ious[category.item()].append(iou[y_mask[category]])\n",
        "\n",
        "    # Compute mean IoU.\n",
        "    ious = [torch.stack(iou).mean(0).mean(0) for iou in ious]\n",
        "    return torch.tensor(ious).mean().item(), sum(losses)/len(losses), sum(accs)/len(accs)\n",
        "\n",
        "\n",
        "def run(train_dataset, test_dataset, model, epochs, batch_size, lr,\n",
        "        lr_decay_factor, lr_decay_step_size, weight_decay):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  \n",
        "    duration = 0\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_accs = []\n",
        "    test_ious = []\n",
        "    model = model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_start = time.perf_counter()\n",
        "\n",
        "        train_loss = train(model, optimizer, train_loader, device)\n",
        "        train_losses.append(train_loss.item())\n",
        "        iou, test_acc, test_loss = test(model, test_loader, device)\n",
        "        #test_loss = test(model, test_loader, device)\n",
        "        test_ious.append(iou)\n",
        "        test_accs.append(test_acc)\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_end = time.perf_counter()\n",
        "        duration += t_end - t_start\n",
        "        print('Epoch: {:03d}, Test: {:.4f}, Duration: {:.2f}'.format(\n",
        "            epoch, test_acc, t_end - t_start))\n",
        "\n",
        "        '''if epoch % lr_decay_step_size == 0:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_decay_factor * param_group['lr']'''\n",
        "\n",
        "    return duration, train_losses, test_losses, test_accs, test_ious"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-3uFWllzkhs"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = XConv(0, 256, dim=3, kernel_size=8, hidden_channels=256)\n",
        "        self.conv2 = XConv(256, 256, dim=3, kernel_size=12, hidden_channels=256, dilation=2)\n",
        "        self.conv3 = XConv(256, 512, dim=3, kernel_size=16, hidden_channels=512, dilation=2)\n",
        "        self.conv4 = XConv(512, 1024, dim=3, kernel_size=16, hidden_channels=1024, dilation=6)\n",
        "        self.conv5 = XConv(1024, 512, dim=3, kernel_size=16, hidden_channels=512, dilation=6)\n",
        "        self.conv6 = XConv(512, 256, dim=3, kernel_size=12, hidden_channels=256, dilation=6)\n",
        "        self.conv7 = XConv(256, 256, dim=3, kernel_size=8, hidden_channels=256, dilation=6)\n",
        "        self.conv8 = XConv(256, 256, dim=3, kernel_size=8, hidden_channels=256, dilation=4)\n",
        "\n",
        "        self.lin1 = Lin(256, 256)\n",
        "        self.lin2 = Lin(256, 256)\n",
        "        self.lin3 = Lin(256, num_classes)\n",
        "\n",
        "    def forward(self, pos, batch):\n",
        "        x1 = F.relu(self.conv1(None, pos, batch))\n",
        "\n",
        "        idx = fps(pos, batch, ratio=0.375)\n",
        "        x, pos, batch = x1[idx], pos[idx], batch[idx]\n",
        "\n",
        "        x2 = F.relu(self.conv2(x, pos, batch))\n",
        "\n",
        "        idx = fps(pos, batch, ratio=0.334)\n",
        "        x, pos, batch = x2[idx], pos[idx], batch[idx]\n",
        "\n",
        "        x3 = F.relu(self.conv3(x, pos, batch))\n",
        "        x = F.relu(self.conv4(x3, pos, batch))\n",
        "        x = F.relu(self.conv5(x+x3, pos, batch))\n",
        "        x = F.relu(self.conv6(x+x2, pos, batch))\n",
        "        x = F.relu(self.conv7(x+x1, pos, batch))\n",
        "        x = F.relu(self.conv8(x+x1, pos, batch))\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin3(x)\n",
        "        return F.log_softmax(x, dim=-1)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTmncwEXrx9i"
      },
      "source": [
        "train_dataset, test_dataset = get_dataset(name='ShapeNet', num_points=None)\n",
        "model = Net(train_dataset.num_classes)\n",
        "\n",
        "duration, train_losses, test_losses, test_accs, test_ious = run(train_dataset, test_dataset, model, EPOCHS, BATCH_SIZE, LR, LR_DECAY_FACTOR, LR_DECAY_STEP_SIZE, WEIGHT_DECAY)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}